{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d3c6ba5-1964-4612-91ba-314491eac67f",
   "metadata": {},
   "source": [
    "# Tesisquare Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbbd9c6-1caa-4b21-b01b-2a02752554f5",
   "metadata": {},
   "source": [
    "## A data science project by Porsche GTRR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fa181e-5c6b-48e5-a590-45f133b8731d",
   "metadata": {},
   "source": [
    "We are the Porsche GTRR a team formed by Grosso Luca, Torterolo Francesco, Risso Beatrice e Robresco Simone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713dc636-7fb2-45c6-9583-a46e7ff561f9",
   "metadata": {},
   "source": [
    "# Tesisquare Dataset Analysis Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c61658c-4719-4d04-8c5a-ae15e2467328",
   "metadata": {},
   "source": [
    "### Roles:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69ac8e4-a47f-4d5e-a182-3db9494e1f94",
   "metadata": {},
   "source": [
    "- __Graphics Designer:__ Luca Grosso\n",
    "- __Coders:__ Simone Robresco, Francesco Torterolo, Beatrice Risso\n",
    "- __Code reviewer:__ Luca Grosso, Beatrice Risso\n",
    "- __Notebook Redactor:__ Beatrice Risso, Luca Grosso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa43b14-91b9-4de5-881a-2438e461dfed",
   "metadata": {},
   "source": [
    "## Project Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f0a159-3a17-4e57-b51a-53bfe03da198",
   "metadata": {},
   "source": [
    "1. __Introduction:__ Overview of the project goals and team roles.\n",
    "2. __Dataset Description:__ Summary of the dataset provided by Tesisquare and its key features.\n",
    "3. __Exploratory Data Analysis (EDA):__ High-level exploration of trends and patterns.\n",
    "4. __Conclusions:__ Insights derived from the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919b27b1-455b-480a-ab8a-86083547923b",
   "metadata": {},
   "source": [
    "## CSV Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ff49fa-f953-4dc3-a538-c5d4d24de795",
   "metadata": {},
   "source": [
    "- __SERVICETYPE:__  type of service or vehicle used for the delivery\n",
    "- __VEHICLETYPE:__ type of vehicle\n",
    "- __DEPARTURE_COUNTRY:__ country of departure\n",
    "- __DEPARTURE_ZIPCODE:__ zipocde (CAP) of departure\n",
    "- __ARRIVAL_COUNTRY:__ country of arrival\n",
    "- __ARRIVAL_ZIPCODE:__ zipcode (CAP) of arrival"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e803bb7-6ff6-4f6d-af37-5a8dd98ab4a1",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89e77b0-7bda-400f-8ed2-9d24d0a26239",
   "metadata": {},
   "source": [
    "In this project, our team analyzed a dataset provided by the company Tesisquare related to logistics to analyze the data and the relationships between the different features of the dataset. Specifically, we examined the connections between: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be93cf57-6bbc-477a-b75a-910cd0589737",
   "metadata": {},
   "source": [
    "Simone Robresco, Francesco Torterolo, and Beatrice Risso developed the code for data analysis and visualization. Simone Robresco and Luca Grosso designed the types of charts used and the website. Beatrice Risso and Luca Grosso organized the notebook and customized the project presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376994df-e95d-4b8d-b36d-cc0b934609a1",
   "metadata": {},
   "source": [
    "# Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e88a3b-c889-41c2-af01-0e25f2878c3d",
   "metadata": {},
   "source": [
    "The dataset provides logistics data from a company that handles intercontinental shipments, including:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9727c4a-cf55-482a-a2bb-022ae554c433",
   "metadata": {},
   "source": [
    "- Logistics details (e.g., mode of transport used, services utilized, shipping day and date). \n",
    "- Travel-related information (e.g., departure and arrival countries, departure and arrival zip codes, distance in km).\n",
    "- Information related to the transported products (e.g., net and gross weight, volume). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e67825-d81a-4345-9d4e-8b3fa0f36946",
   "metadata": {},
   "source": [
    "Understanding the data's structure and cleaning it is crucial before diving into the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3268c2-3621-4767-af01-5d4363032a63",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f293009c-de50-4cab-b933-02c7e0e7a026",
   "metadata": {},
   "source": [
    "## __Goals of the Analysis__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688b9915-a8b5-453b-9551-50d9cc196a2e",
   "metadata": {},
   "source": [
    "The main objective of this analysis is to understand the relationships between the characteristics of the shipments and the delivery times. Key areas of interest include:\n",
    "\n",
    "- The difference in vehicles and services.\n",
    "- The departure and arrival countries\n",
    "- The shipping date.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f4ce5e-b7c9-4543-b5d8-3e7279b0f8e9",
   "metadata": {},
   "source": [
    "### Importation of libraries and upload of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090f5b73-3342-4f7e-a229-84303ce8f578",
   "metadata": {},
   "source": [
    "In this cell we are importing all the libraries that will be necessary for all the different snippets,\n",
    "and than upload the dataset via an xls file that we gained before using the API on the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e94d93-aaec-42e8-84d9-4c0d3b75b0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import requests\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03488fc-524b-414e-938a-fc946e96ca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload of the dataset\n",
    "file_path = './delivery_data.xls'\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83cd22f-64cf-46f2-bdf1-04029caaedfe",
   "metadata": {},
   "source": [
    "### Basic analysis of the features and values of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0080cd-644b-4a24-96d8-0e87bc4ec65c",
   "metadata": {},
   "source": [
    "In the next three cells we analyzed the dataset to understand how to work on it, specially trying to understand\n",
    "how many NULL data for each feature we had"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b8af92-b737-429d-8b1e-8e6ad7a8ed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the first rows of the dataset\n",
    "print(\"First rows of the dataset:\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41873f2-46af-4143-94d4-e632b493d1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "print(\"\\nDescriptive statistics:\")\n",
    "data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d4db44-5c07-4743-afa8-34557d6bd63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset size:\", data.shape)  # Dataset's dimension\n",
    "data.info()  # General information about data types and counting NON-null values\n",
    "\n",
    "# We calculate the number of missing values for each column\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "# Printing of missing data in different columns \n",
    "print(\"Columns with missing data\")\n",
    "missing_values[missing_values > 0]  # We only show columns with missing values\n",
    "\n",
    "\n",
    "# List of specific columns to analyze\n",
    "colonne_selezionate = [\n",
    "    'SERVICETYPE',\n",
    "    'VEHICLETYPE',\n",
    "    'DEPARTURE_COUNTRY',\n",
    "    'ARRIVAL_COUNTRY',\n",
    "    'GROSS_WEIGHT_KG',\n",
    "    'NET_WEIGHT_KG',\n",
    "    'VOLUME_M3',\n",
    "    'DECLARED_DISTANCE_KM',\n",
    "    'DELIVERY_TIME_HH',\n",
    "    'WDAY'\n",
    "]\n",
    "\n",
    "# Displaying the count for each selected column\n",
    "for column in colonne_selezionate:\n",
    "    print(f\"\\nCount values for the column '{column}':\")\n",
    "    print(data[column].value_counts())\n",
    "    print(\"-\" * 50)  # Separation line for better readability\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d6127e-c193-40f4-9a1d-94ab42578376",
   "metadata": {},
   "source": [
    "### Count of shippings arrival and departure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cdfe72-d937-4d65-bd47-0b8a6a984b04",
   "metadata": {},
   "source": [
    "Than we tried to find out how many deliveries we had from various combination of start point and arrival point:\n",
    "\n",
    "- from US to Everywhere\n",
    "- from US to Everywhere (listed based on the country of arrival)\n",
    "- from US to US\n",
    "- from US to IT\n",
    "- from IT to Everywhere\n",
    "- from IT to Everywhere (listed based on the country of arrival)\n",
    "- from IT to US\n",
    "- from IT to IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30bc7d5-5b55-4009-96eb-7b89ff1847ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count shipments from the United States (US)\n",
    "us_shipments_count = data[data['DEPARTURE_COUNTRY'] == 'US'].shape[0]\n",
    "print(f\"\\nNumber of shipments from US: {us_shipments_count}\")\n",
    "\n",
    "\n",
    "# Shipment count by country of arrival, only for those departing from the US\n",
    "arrival_counts_us = data[data['DEPARTURE_COUNTRY'] == 'US']['ARRIVAL_COUNTRY'].value_counts()\n",
    "print(\"\\nShipment count by country of arrival (shipments from US only):\")\n",
    "print(arrival_counts_us)\n",
    "\n",
    "\n",
    "# Total shipments from US to IT\n",
    "us_to_it_count = data[(data['DEPARTURE_COUNTRY'] == 'US') & (data['ARRIVAL_COUNTRY'] == 'IT')].shape[0]\n",
    "print(f\"Number of shipments from US to IT: {us_to_it_count}\")\n",
    "\n",
    "\n",
    "# Total shipments from US to US\n",
    "us_to_us_count = data[(data['DEPARTURE_COUNTRY'] == 'US') & (data['ARRIVAL_COUNTRY'] == 'US')].shape[0]\n",
    "print(f\"Number of shipments from US to US: {us_to_us_count}\")\n",
    "\n",
    "\n",
    "# Count shipments from Italy (IT)\n",
    "it_shipments_count = data[data['DEPARTURE_COUNTRY'] == 'IT'].shape[0]\n",
    "print(f\"\\nNumber of shipments from IT: {it_shipments_count}\")\n",
    "\n",
    "\n",
    "# Shipment count by country of arrival, only for those departing from IT\n",
    "arrival_counts_it = data[data['DEPARTURE_COUNTRY'] == 'IT']['ARRIVAL_COUNTRY'].value_counts()\n",
    "print(\"\\nShipment count by country of arrival (departments from IT only):\")\n",
    "print(arrival_counts_it)\n",
    "\n",
    "\n",
    "# Total shipments from IT to US\n",
    "it_to_us_count = data[(data['DEPARTURE_COUNTRY'] == 'IT') & (data['ARRIVAL_COUNTRY'] == 'US')].shape[0]\n",
    "print(f\"Number of shipments from IT to US: {it_to_us_count}\")\n",
    "\n",
    "\n",
    "# Total shipments from IT to IT\n",
    "it_to_it_count = data[(data['DEPARTURE_COUNTRY'] == 'IT') & (data['ARRIVAL_COUNTRY'] == 'IT')].shape[0]\n",
    "print(f\"Number of shipments from IT to IT: {it_to_it_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164bfc09-6e85-4b61-b72e-b861ee69fd46",
   "metadata": {},
   "source": [
    "Here we created some graphs in order to visualize, not extremely accurately, the distribution of the different type of data.\n",
    "Than we cleaned the dataset removing the empty rows and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b55d06-4ad0-4227-b530-e9760df79f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of specific numeric columns to analyze\n",
    "colonne_numeriche_selezionate = [\n",
    "    'GROSS_WEIGHT_KG',\n",
    "    'NET_WEIGHT_KG',\n",
    "    'VOLUME_M3',\n",
    "    'DECLARED_DISTANCE_KM',\n",
    "    'DELIVERY_TIME_HH',\n",
    "    'WDAY'\n",
    "]\n",
    "\n",
    "# Create histograms for selected columns only\n",
    "plt.figure(figsize=(20, 20))\n",
    "data[colonne_numeriche_selezionate].hist(bins)\n",
    "plt.suptitle(\"Distribution of the selected numerical variables\")\n",
    "plt.tight_layout()  # Automatically adjust gaps between subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d3b1c5-ae87-42d3-afce-8a88a355deab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows without null values\n",
    "data_no_nulls = data.dropna()\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "output_file_path = './delivery_clean.csv'\n",
    "data_no_nulls.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"New CSV created with rows without null values: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0bbfa9-dc73-4ad5-8ebf-5b0b8f7b6dc1",
   "metadata": {},
   "source": [
    "# Prediction of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a66e23f-b45c-4752-99c7-9cb597da8dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the CSV file\n",
    "df = pd.read_csv(\"./delivery_clean.csv\")\n",
    "\n",
    "# Normalize column names\n",
    "df.columns = df.columns.str.strip().str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06df3661-ab1d-4f8f-b0b4-55497fc1894a",
   "metadata": {},
   "source": [
    "We created a function to update the vehicle type base on the service type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256e4ae5-4e3c-475b-8aa4-47406466568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update VEHICLETYPE based on SERVICETYPE\n",
    "def update_vehicletype(row):\n",
    "    if pd.isna(row['VEHICLETYPE']) or row['VEHICLETYPE'] not in ['express', 'standard']:\n",
    "        if 'corriere espresso' in str(row['SERVICETYPE']).lower():\n",
    "            return 'express'\n",
    "        else:\n",
    "            return 'standard'\n",
    "    return row['VEHICLETYPE']\n",
    "\n",
    "# Apply function on 'VEHICLETYPE' column\n",
    "df['VEHICLETYPE'] = df.apply(update_vehicletype, axis=1)\n",
    "\n",
    "# Save the updated file\n",
    "df.to_csv(\"./filtered_delivery_data.csv\", index=False)\n",
    "\n",
    "print(\"Updated file saved as 'filtered_delivery_data.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438c4cd9-b395-4282-a990-51fbbbca08c4",
   "metadata": {},
   "source": [
    "Than search the unique type of each feature and upload them in the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e9c758-51c2-4238-a933-03442a44f5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find unique types and their count\n",
    "unique_vehicle_types = df['VEHICLETYPE'].nunique()\n",
    "all_vehicle_types = df['VEHICLETYPE'].unique()\n",
    "\n",
    "print(f\"Number of different types of vehicles: {unique_vehicle_types}\")\n",
    "print(f\"Types of vehicles present: {all_vehicle_types}\")\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"./delivery_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aba285-28ac-4423-aaae-9315b54f2f76",
   "metadata": {},
   "source": [
    "Here we searched for the specific charateristics and then defined a function that let us predict the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090d55e0-8d38-4070-a68d-82406614129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical characteristics for prediction\n",
    "features = ['GROSS_WEIGHT_KG', 'NET_WEIGHT_KG', 'VOLUME_M3', 'DECLARED_DISTANCE_KM', 'DELIVERY_TIME_HH']\n",
    "\n",
    "def predict_missing_values(df, target_column):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Predictive analytics for: {target_column}\")\n",
    "    print('='*50)\n",
    "    \n",
    "    df_known = df.dropna(subset=[target_column])\n",
    "    df_unknown = df[df[target_column].isna()]\n",
    "    \n",
    "    if df_known.empty or df_unknown.empty:\n",
    "        print(f\"No value to predict for {target_column}\")\n",
    "        return df\n",
    "    \n",
    "    X = df_known[features]\n",
    "    y = df_known[target_column].astype('category').cat.codes\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Model evaluation\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\nAccuracy metrics for {target_column}:\")\n",
    "    print(f\"Overall accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    print(\"\\nDetailed classification report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': features,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nImportance of features:\")\n",
    "    for idx, row in feature_importance.iterrows():\n",
    "        print(f\"{row['feature']}: {row['importance']:.4f}\")\n",
    "    \n",
    "    # Predicting missing values\n",
    "    df_unknown[target_column] = model.predict(df_unknown[features])\n",
    "    df = pd.concat([df_known, df_unknown])\n",
    "    \n",
    "    print(f\"\\nNumber of predicted values: {len(df_unknown)}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8301b1a7-17f5-4c4b-a6fe-728f5f766baf",
   "metadata": {},
   "source": [
    "Than we execute the predictions and upload them on a new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7469fd63-cf91-4830-9ee4-81e7c2190e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run predictions\n",
    "print(\"Starting predictive analytics...\")\n",
    "df = predict_missing_values(df, 'VEHICLETYPE')\n",
    "df = predict_missing_values(df, 'SERVICETYPE')\n",
    "print(\"\\nRegression completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f74e73-a6f5-48f4-87d2-b1c3a9eab5b9",
   "metadata": {},
   "source": [
    "# Data cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cc7472-77db-46da-ae40-14a9e794ff0a",
   "metadata": {},
   "source": [
    "Here we upload the file obtained from the Prediction code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dee5e8-d9c6-4b86-8db2-bdec257ed844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the original CSV\n",
    "df_originale = pd.read_csv('./filtered_delivery_data.csv')\n",
    "\n",
    "print(df_originale.columns)\n",
    "df_originale.columns = df_originale.columns.str.strip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c368f4ce-5fe6-47ef-a60f-2c08072f31ab",
   "metadata": {},
   "source": [
    "Than we extracted the month from the shipping date and calculated the amount of shippings in each month, after that we calculated the amount of times each service type appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f16074-90ea-49bb-9a20-a9ed281e2fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose the column is called 'SHIPPING_DATE'\n",
    "# Extract only the month (6th and 7th characters)\n",
    "df_originale['SHIPPING_DATE'] = df_originale['SHIPPING_DATE'].str[5:7]\n",
    "\n",
    "# Print the result to check\n",
    "print(df_originale['SHIPPING_DATE'])\n",
    "\n",
    "# Count how many times it appears each month\n",
    "conteggio_mesi = df_originale['SHIPPING_DATE'].value_counts()\n",
    "\n",
    "# Prints the month count\n",
    "print(conteggio_mesi)\n",
    "\n",
    "# Count how many times each type of SERVICETYPE appears\n",
    "conteggio_service_type = df_originale['SERVICETYPE'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7474df3-c679-4b75-9a77-c1bc2242561b",
   "metadata": {},
   "source": [
    "Here we create the graph to show the spread of each service type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d2064d-639b-4577-8be4-765d0df6d63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pie chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(conteggio_service_type, labels=conteggio_service_type.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Distribution of SERVICETYPE types')\n",
    "plt.axis('equal')  # To maintain the circular shape\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8af6317-676f-4741-9f3e-cbb005a9764f",
   "metadata": {},
   "source": [
    "We count the amount of times a zipcode appears and then we calculate the percentage of each zipcode.\n",
    "After this we filter the zipcode with a 2 or less percent of appearence and group them under the \"others\" definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7549e18b-5dfa-4f55-955b-9da455a55517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many times each zip code appears in the 'DEPARTURE_ZIPCODE' column\n",
    "conteggio_departure_zipcode = df_originale['DEPARTURE_ZIPCODE'].value_counts()\n",
    "\n",
    "# Calculate the percentage for each zip code\n",
    "percentuale_zipcode = conteggio_departure_zipcode / conteggio_departure_zipcode.sum() * 100\n",
    "\n",
    "# Filter zip codes with percentage greater than 2%\n",
    "zipcodes_frequenti = percentuale_zipcode[percentuale_zipcode > 2]\n",
    "\n",
    "# Groups all other postcodes into the \"Other\" category\n",
    "other = percentuale_zipcode[percentuale_zipcode <= 2].sum()\n",
    "zipcodes_frequenti['Others'] = other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eac86a0-2616-4ab4-a05f-fac74a9ee456",
   "metadata": {},
   "source": [
    "We created a pie graph to rappresent the diffusion of each zipcode in order to understand where was the primary start point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf67434-0fc0-468a-a607-2b160d7f170d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pie chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(zipcodes_frequenti, labels=zipcodes_frequenti.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Departure ZIP Code Distribution (DEPARTURE_ZIPCODE) - Greater than 2%')\n",
    "plt.axis('equal')  # To maintain the circular shape\n",
    "plt.show()\n",
    "\n",
    "df = pd.read_csv('./filtered_delivery_data.csv')\n",
    "\n",
    "# Find unique types and their count\n",
    "unique_vehicle_types = df_originale['VEHICLETYPE'].nunique()\n",
    "all_vehicle_types = df_originale['VEHICLETYPE'].unique()\n",
    "\n",
    "print(f\"Number of different types of vehicles: {unique_vehicle_types}\")\n",
    "print(f\"Types of vehicles present: {all_vehicle_types}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d18860-159f-4ede-863a-33e4eb0bc0a2",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6404883-c66d-465e-a302-c66ec4832392",
   "metadata": {},
   "source": [
    "We found out that there is a huge amount of deliveries that comes and goes from Italy compared to the amount from the US and there are none from Italy to the US.\n",
    "We also noticed that more deliveries starts from the first days of the week, we recommend to try to analyze this phenomenom in order to understand and project a optimized way to spread those deliveries in the rest of the week.\n",
    "Another thing we saw it's that most of the deliveries comes from a specific zipcode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fd9f4f-4182-40e7-99ef-6d13f3303350",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
